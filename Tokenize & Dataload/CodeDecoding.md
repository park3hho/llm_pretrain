# [CD] 1. Multi-Head Attention
Link: https://velog.io/@park2do/%EC%BD%94%EB%93%9C-%ED%8C%8C%ED%97%A4%EC%B9%98%ED%82%A4-Multi-Head-Attention

Transformerì˜ í•µì‹¬ êµ¬ì„±ìš”ì†Œì¸ â€œMulti-Head Attentionâ€ (ë‹¤ì¤‘ í—¤ë“œ ì–´í…ì…˜) ì„ PyTorchë¡œ ì§ì ‘ êµ¬í˜„í•œ í´ë˜ìŠ¤.

## ğŸ§© ì „ì²´ êµ¬ì¡° ìš”ì•½
```
class MultiHeadAttention(nn.Module):
    def __init__(self, d_in, d_out):  # ì´ˆê¸°í™”
        ...
    def forward(self, x):  # ìˆœì „íŒŒ
        ...
```
Transformerì—ì„œ ì…ë ¥ x (ì˜ˆ: í† í° ì„ë² ë”©)ì„ ë°›ì•„
â€œë‹¨ì–´ ê°„ì˜ ì—°ê´€ì„±(Attention)â€ì„ ê³„ì‚°í•˜ê³ ,
ê·¸ ê²°ê³¼ë¥¼ ë‹¤ì‹œ ì„ë² ë”© í˜•íƒœë¡œ ëŒë ¤ì£¼ëŠ” ëª¨ë“ˆ

## í´ë˜ìŠ¤ ì •ì˜ ë° ì´ˆê¸°í™” ë¶€ë¬¸
### 1. ì¸ì
``` ì¸ì
class MultiHeadAttention(nn.Module):
    def __init__(self, d_in, d_out):
        super().__init__()
```
- `d_in`: ì…ë ¥ ì°¨ì› (ì…ë ¥ ë²¡í„°ì˜ feature ìˆ˜)
- `d_out`: ì¶œë ¥ ì°¨ì› (ì „ì²´ í—¤ë“œë“¤ì„ í•©ì¹œ ì°¨ì›)
- `super().__init__()`ë¡œ `nn.Module` ì´ˆê¸°í™”.

> nn.Moduleì´ ë­”ë°?
>- nn.Moduleì€ PyTorchì—ì„œ ì‹ ê²½ë§(Neural Network)ì„ ë§Œë“¤ ë•Œ ì‚¬ìš©í•˜ëŠ” **ëª¨ë“  ëª¨ë¸ì˜ ê¸°ë³¸ í´ë˜ìŠ¤(base class)**

### 2. í—¤ë“œ ê°œìˆ˜ í• ë‹¹
``` 
assert d_out % NUM_HEADS == 0, "d_out must be divisible by n_heads"
```
- d_outì´ NUM_HEADSë¡œ ë‚˜ëˆ„ì–´ ë–¨ì–´ì§€ëŠ”ì§€ í™•ì¸. ê° í—¤ë“œì˜ ì°¨ì›(head_dim) = d_out // NUM_HEADS.
- d_outì€ ì¶œë ¥ ì°¨ì› ìˆ˜ (ì˜ˆ: 512)
- NUM_HEADSëŠ” ì–´í…ì…˜ í—¤ë“œì˜ ê°œìˆ˜ (ì˜ˆ: 8)
- ê° í—¤ë“œê°€ ì²˜ë¦¬í•  ì°¨ì›ì„ ë™ì¼í•˜ê²Œ ë‚˜ëˆ„ê¸° ìœ„í•´ d_outì´ NUM_HEADSë¡œ ë‚˜ëˆ„ì–´ë–¨ì–´ì ¸ì•¼ í•¨
- ì»´í“¨íŒ… ìì› ì—¬ìœ ê°€ ìˆìœ¼ë©´ í¬ê²Œ.

### 3. Dimension
``` dim
self.d_out = d_out
self.head_dim = d_out // NUM_HEADS
```
- ë‚´ë¶€ ì €ì¥. `head_dim`ì€ í•œ í—¤ë“œê°€ ê°€ì§€ëŠ” feature ìˆ˜
- í•˜ë‚˜ì˜ ì–´í…ì…˜ í—¤ë“œê°€ ë‹´ë‹¹í•˜ëŠ” ì°¨ì› í¬ê¸°
- ì˜ˆ: d_out=512, NUM_HEADS=8 â†’ head_dim=64

### 4. QKV ê°€ì¤‘ì¹˜ í–‰ë ¬
``` Weight Matrix
self.W_query = nn.Linear(d_in, d_out, bias=QKV_BIAS)
self.W_key = nn.Linear(d_in, d_out, bias=QKV_BIAS)
self.W_value = nn.Linear(d_in, d_out, bias=QKV_BIAS)
```
- ì…ë ¥ ë²¡í„° xë¥¼ ê°ê° Query, Key, Valueë¡œ ë³€í™˜í•˜ëŠ” ì„ í˜• ë³€í™˜ (ê°€ì¤‘ì¹˜ í–‰ë ¬).
- ì¦‰, ì…ë ¥ ë¬¸ì¥ì˜ ê° ë‹¨ì–´ë¥¼ 3ê°€ì§€ ì—­í• ë¡œ ë§¤í•‘í•˜ëŠ” ê³¼ì •.
- `QKV_BIAS`ëŠ” ì „ì—­ ìƒìˆ˜ë¡œ bias ì‚¬ìš© ì—¬ë¶€(ì°¸/ê±°ì§“)

### 5. ì„ í˜• 
``` out_proj, Dropout
self.out_proj = nn.Linear(d_out, d_out)
self.dropout = nn.Dropout(DROP_RATE)
```
- ì—¬ëŸ¬ í—¤ë“œì˜ ì¶œë ¥ì„ ë‹¤ì‹œ í•©ì³ì¸µ ìµœì¢… ì¶œë ¥ìœ¼ë¡œ ë³€í™˜ / ì—¬ëŸ¬ í—¤ë“œë¥¼ í•©ì¹œ ê²°ê³¼ì—
ë§ˆì§€ë§‰ìœ¼ë¡œ ì ìš©ë˜ëŠ” ì¶œë ¥ íˆ¬ì‚¬ 
- Dropoutìœ¼ë¡œ ì¼ë¶€ ì—°ê²°ì„ ëŠì–´ ê³¼ì í•© ë°©ì§€(regularization)
- í›ˆë ¨ ë°ì´í„°ì— ê³¼ë„í•˜ê²Œ ë§ì¶”ì–´ì ¸ ìˆì–´ì„œ(ì´ ë¬¸ì œë¥¼ overfittingì´ë¼ê³  ì¼ì»«ìŒ) ì¼ë°˜í™” ëŠ¥ë ¥ì´ ë–¨ì–´ì§
- ì´ë¡œ ì¸í•´ "ìƒˆë¡œìš´ ë¬¸ì¥"ì—ì„œ ì„±ëŠ¥ì´ ë–¨ì–´ì§ˆ ìˆ˜ë„ ìˆìŒ ê·¸ë˜ì„œ Dropoutì„ ì‚¬ìš©.

> `d_model`ì´ ë¬´ìŠ¨ ì˜ë¯¸ì¸ë°?
>- d_modelì€ **Multi-Head Attention**ì˜ ì „ì²´ ì°¨ì›ì„ ì˜ë¯¸í•¨.

### 6. ë§ˆìŠ¤í‚¹
``` 
self.register_buffer('mask', torch.triu(torch.ones(CONTEXT_LENGTH, CONTEXT_LENGTH), diagonal=1))
```
- ìƒì‚¼ê°í–‰ë ¬(ìœ„ìª½ì´ 1, ì•„ë˜ëŠ” 0)
- â€œë¯¸ë˜ í† í°ì„ ë³´ì§€ ëª»í•˜ê²Œâ€ í•˜ëŠ” ìºì£¼ì–¼ ë§ˆìŠ¤í¬ (causal mask) (GPT ê°™ì€ ì–¸ì–´ëª¨ë¸ì—ì„œ ì¤‘ìš”)

ì˜ˆì‹œ (ê¸¸ì´ 4):
[[0, 1, 1, 1],
 [0, 0, 1, 1],
 [0, 0, 0, 1],
 [0, 0, 0, 0]]

    
## ìˆœì „íŒŒ (foward)
### 1. ì…ë ¥ í˜•íƒœ
```
b, num_tokens, d_in = x.shape
```
- b: batch size [í•œ ë²ˆì— ì²˜ë¦¬í•˜ëŠ” ë¬¸ì¥ ê°œìˆ˜, not í† í°ì˜ ê°œìˆ˜]
- num_tokens: í•œ ë¬¸ì¥(ì‹œí€€ìŠ¤)ì˜ í† í° ìˆ˜
- d_in: ì…ë ¥ ì„ë² ë”© ì°¨ì› > ê³„ì‚° ìœ„ë„
> ë³´í†µ ëª‡ê°œì˜ featureë¡œ ì´ë£¨ì–´ì ¸ìˆëŠ”ë°?

| ëª¨ë¸ ì¢…ë¥˜            | ì…ë ¥ ì„ë² ë”© ì°¨ì›(d_in or d_model) | ë¹„ê³                        |
| ---------------- | -------------------------- | ------------------------ |
| ì†Œí˜• ëª¨ë¸ (toy, ì‹¤ìŠµìš©) | 64 ~ 256                   | GPU/CPUì—ì„œë„ ë¹ ë¥´ê²Œ í•™ìŠµ ê°€ëŠ¥     |
| ì¤‘í˜• ëª¨ë¸ (ê¸°ì´ˆ NLP)   | 512 ~ 1024                 | BERT base, GPT-2 small ë“± |
| ëŒ€í˜• ëª¨ë¸ (ìƒìš© LLM)   | 2048 ~ 12288               | GPT-3, LLaMA, Falcon ë“±   |
| ë¹„ì „ íŠ¸ëœìŠ¤í¬ë¨¸(ViT)    | 256 ~ 1024                 | ì´ë¯¸ì§€ patch ì„ë² ë”©            |

> ì„ë² ë”© ì°¨ì›ì„ ì…ë ¥ê³¼ ì¶œë ¥ ë‘ê°€ì§€ë¥¼ ë‹¤ë¥´ê²Œ í•´ë„ ë˜ëŠ”ê°€?
>- FFN(Feed Foward Network) ì˜ˆì‹œ
> ```
> self.fc1 = nn.Linear(512, 2048)  # í™•ì¥
> self.fc2 = nn.Linear(2048, 512)  # ì¶•ì†Œ
> ```
>- (1) ë¹„ì„ í˜• ë³€í˜• ê³µê°„ì„ ë„“í˜€ì„œ í‘œí˜„ë ¥ ì¦ê°€ (í™•ì¥)
>- (2) ì •ë³´ í™•ì¥ & ì¶”ìƒí™” ë‹¨ê³„ (ì‘ìš©)
>- (3) í•™ìŠµ ì•ˆì •í™” (ì¶•ì†Œ)

### 2. Q, K, V Mapping
``` QKV Mapping 
keys = self.W_key(x)
queries = self.W_query(x)
values = self.W_value(x)
```
- ì…ë ¥ ë¬¸ì¥ì˜ ê° ë‹¨ì–´ë¥¼ Query, Key, Value ë²¡í„°ë¡œ ë§¤í•‘

### 3. ì—¬ëŸ¬ í—¤ë“œë¡œ ë¶„ë¦¬
```
keys = keys.view(b, num_tokens, NUM_HEADS, self.head_dim)
queries = queries.view(b, num_tokens, NUM_HEADS, self.head_dim)
values = values.view(b, num_tokens, NUM_HEADS, self.head_dim)
```
- ì´ì œ ê° ë‹¨ì–´ì˜ ì„ë² ë”©ì„ NUM_HEADS ê°œë¡œ ìª¼ê°œê¸°
- ì›ë˜: (b, num_tokens, 512)
- ë°”ë€ í›„: (b, num_tokens, 8, 64)

> ì ê¹ ì™œ ë‚˜ëˆ„ì–´ì•¼ í•˜ëŠ”ê±´ë°?
>- Multi-Head Attentionì˜ ê°€ì¥ í•µì‹¬ì ì¸ ë¶€ë¶„,
>- ê° í—¤ë“œê°€ ê°ìì˜ ë²¡í„° ê³µê°„ì„ ì‚¬ìš©í•˜ì—¬ ë³¸ë‹¤ë©´ ê° ë¶€ë¶„ì—ì„œ ì—­í• ì´ ì •í•´ì§€ê¸° ë•Œë¬¸ì— ë¹„ì„ í˜•ì ì¸ ë°ì´í„°ë“¤ì„ ë¶„ì„ ê°€ëŠ¥í•¨.

### 4. ì°¨ì› ì¬ë°°ì—´ (í—¤ë“œ ê¸°ì¤€ ê³„ì‚°í•˜ê¸° ìœ„í•´)
```
keys = keys.transpose(1, 2)
queries = queries.transpose(1, 2)
values = values.transpose(1, 2)
```
(b, num_tokens, NUM_HEADS, self.head_dim) â†’ (b, NUM_HEADS, num_tokens, head_dim)
>- ì†”ì§íˆ ì™œ ë‚˜ëˆ ì•¼ í•˜ëŠ”ì§€ì— ëŒ€í•œ ì„¤ëª…ì„ ì´í•´ë¥¼ ëª»í–ˆë‹¤. 
>- ê³„ì‚°ì´ ì•ˆëœë‹¤ëŠ”ë° ê·¸ëƒ¥ ì§„í–‰í•´ë„ ë˜ëŠ”ê±° ì•„ë‹Œê°€ ì‹¶ë‹¤ê°€ë„ ê·¸ëƒ¥ ê·¸ëŸ¬ë ¤ë‹ˆí•œë‹¤.
>- ì•„ë¬´íŠ¼ ì´ì œ ê° í—¤ë“œë³„ë¡œ ì–´í…ì…˜ ê³„ì‚° ê°€ëŠ¥.
>- ì•„ ë’¤ì— ê·¸ëƒ¥ 2,3 ì´ë ‡ê²Œ ì¨ì•¼í•´ì„œ ê·¸ëŸ¬ë„¤

### 5. í–‰ë ¬ê³±(Matrix Duplication) - ì–´í…ì…˜ ìŠ¤ì½”ì–´ ê³„ì‚°
```attn_score
attn_scores = queries @ keys.transpose(2, 3)
```

- ì´ ì—°ì‚°ì€ Q Ã— Káµ€ (í–‰ë ¬ê³±) ì…ë‹ˆë‹¤.
- Q í† í°ì´ ë‹¤ë¥¸ í† í°ê³¼ ì–¼ë§ˆë‚˜ ì—°ê´€ë˜ëŠ”ì§€ë¥¼ ë‚˜íƒ€ëƒ„.

`shape: (b, NUM_HEADS, num_tokens, num_tokens)`
> ğŸ”¥ Q Ã— Káµ€ ì˜ ì˜ë¯¸
Matrix element (i, j) = Q[i] Â· K[j]  
>
>â†’ â€œië²ˆì§¸ í† í°ì´ jë²ˆì§¸ í† í°ì— ì–¼ë§ˆë‚˜ ì£¼ëª©í•˜ëŠ”ê°€?â€  
> â†’ Self-attentionì˜ í•µì‹¬ ì˜ë¯¸  
ì—¬ê¸°ì„œ ë‚˜ì˜¨ scoreë¥¼ softmaxí•´ì„œ ê°€ì¤‘ì¹˜ë¡œ ì‚¬ìš©
 
ì°¸ê³  ë§í¬: https://velog.io/@park2do/%ED%96%89%EB%A0%AC%EA%B3%B1-Matrix-Multiplication

### 6. ë§ˆìŠ¤í¬ ì ìš© (ë¯¸ë˜ ì •ë³´ ì°¨ë‹¨)
```
mask_bool = self.mask.bool()[:num_tokens, :num_tokens]
attn_scores.masked_fill_(mask_bool, -torch.inf)
```
ë§ˆìŠ¤í¬ê°€ 1ì¸ ìœ„ì¹˜(=ë¯¸ë˜)ëŠ” -infë¡œ ì±„ì›Œ  
softmax í›„ 0ì´ ë˜ê²Œ í•¨.  
ì¦‰, í˜„ì¬ ë‹¨ì–´ëŠ” ë¯¸ë˜ ë‹¨ì–´ë¥¼ ë³¼ ìˆ˜ ì—†ìŒ.

- ë³´ë©´ ê³ ì¥ë‚¨, ì°½ì˜ì„±ì´ ì•ˆìƒê¹€

### 7. Softmaxë¡œ ê°€ì¤‘ì¹˜ ë³€í™˜
```
attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)
```
- ê°€ì¤‘í•©ì„ 1ë¡œ ë§Œë“¦.
- ë°œìƒí•  ìˆ˜ ìˆëŠ” ë¬¸ì œ ë‘ê°€ì§€: Scale Explosion - Saturation

#### Scale Explosion
ìŠ¤ì¼€ì¼ ê°’ì´ ì ì ˆí•œ ê°’ìœ¼ë¡œ ì¡°ì ˆë˜ì§€ ì•Šì•„ ë°œìƒí•˜ëŠ” ë¬¸ì œ

- ì¼ì–´ë‚˜ëŠ” ë¬¸ì œ
  - softmaxê°€ ê·¹ë‹¨ì ìœ¼ë¡œ ì¹˜ìš°ì¹¨
  - gradient íë¦„ì´ ë¶ˆì•ˆì •í•´ì§

#### Saturation
Softmaxê°€ ë„ˆë¬´ í° ê°’ì¼ ë•Œ ê±°ì˜ 0ê³¼ 1ë§Œ ë‚˜ì˜¤ëŠ” í˜„ìƒ

- ì¼ì–´ë‚˜ëŠ” ë¬¸ì œ
  - ë¯¸ë¶„ê°’ì´ 0ì´ ë¨ (gradient vanishing)
  - í•™ìŠµì´ ì•ˆë¨
  - ì–´í…ì…˜ì´ íŠ¹ì • í† í°ì— **ì™„ì „ ê³ ì •**

#### í•´ê²°ë°©ë²•
  - âˆšdâ‚–ë¡œ ë‚˜ëˆ„ê¸°

### 8. Valueë¥¼ ê°€ì¤‘í•©(Weighted Sum)
```
context_vec = (attn_weights @ values).transpose(1, 2)
```
- ê° í† í°ì˜ â€œë¬¸ë§¥ ì •ë³´â€ë¥¼ ê³„ì‚°.
- (b, num_tokens, NUM_HEADS, head_dim)
> "QK"ë¡œ ìœ ì‚¬ë„ ê³„ì‚°ì„ í–ˆëŠ”ë° ì™œ ë‹¤ì‹œ V(value)ì™€ ê³±í•´ì„œ ìµœì¢… ì¶œë ¥ì„ ë§Œë“œëŠ” ê±°ì§€???
> 
> QKáµ€ëŠ” ìœ ì‚¬ë„(Attention_Score)ì„ êµ¬í•˜ëŠ” ë‹¨ê³„ -> ê° í† í°(ë‹¨ì–´) ê°„ì˜ ì—°ê´€ì„±ì„ íŒŒì•…í•˜ëŠ” ë‹¨ê³„
>> - Q: ë‚´ê°€ ë­˜ ì°¾ê³  ì‹¶ì€ê°€
>> - K: ìƒëŒ€ëŠ” ì–´ë– í•œ ì •ë³´ë¥¼ ê°–ê³  ìˆëŠ”ê°€
>> - QKáµ€[i, j]ëŠ” í† í° iê°€ jë¥¼ ì–¼ë§ˆë‚˜ ì°¸ê³ í•´ì•¼ í•˜ëŠ”ê°€?  
>
> ì¦‰, VëŠ” ì‹¤ì œ ë¬¸ë§¥ì„ ì˜ë¯¸í•˜ë©°, QKáµ€ë¥¼ Vì™€ ê°€ì¤‘í•© í•˜ëŠ” ê²ƒì€ **"ì‹¤ì œ ë¬¸ë§¥ì„ ë¬¸ë§¥ ìœ ì‚¬ë„ë¡œ
> ë¶„ì„"**í•˜ëŠ” ê²ƒì´ë¼ê³  í•´ì„í•˜ë©´ë¨.

### 9. ì—¬ëŸ¬ í—¤ë“œ ê²°ê³¼ í•©ì¹˜ê¸°
```
context_vec = context_vec.reshape(b, num_tokens, self.d_out)
context_vec = self.out_proj(context_vec)
```
- í—¤ë“œë³„ ì¶œë ¥ì„ í•©ì³ ì›ë˜ ì°¨ì›ìœ¼ë¡œ ë˜ëŒë¦¼.
- ìµœì¢… ì¶œë ¥ shape: (b, num_tokens, d_out)

# [CD] 2. Layer Normalization

## Class ì„¤ëª… ë° ì´ˆê¸°í™” ë¶€ë¶„
### 1. Class ì„¤ëª…
``` Class ì„¤ëª…
class LayerNorm(nn.Module):
```
- nn.Moduleì„ ìƒˆë¡œ ìƒì†ë°›ì•„ ìƒˆë¡œìš´ PyTorch ë ˆì´ì–´ë¥¼ ì •ì˜í•¨.  
- ì…ë ¥ í…ì„œì˜ ë§ˆì§€ë§‰ ì°¨ì›ì— ëŒ€í•œ ì •ê·œí™”ë¥¼ ìˆ˜í–‰.

### 2. ì´ˆê¸°í™” ë¶€ë¬¸
```commandline
    def __init__(self, emb_dim):
        super().__init__()
```
- `emb_dim`ì€ **ì…ë ¥ ë²¡í„°ì˜ ì°¨ì›**
- super().__init__()ëŠ” ë¶€ëª¨ í´ë˜ìŠ¤(nn.Module)ì˜ ì´ˆê¸°í™” í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤. PyTorch ëª¨ë¸ ì •ì˜ì—ì„œ í•„ìˆ˜

>- emb_dimì´ ìœ„ì—ì„œ Multi-Head Attentionì—ì„œ ì •ì˜í•œ dimì´ë‘ ê°™ì€ ìˆ˜ì¸ê±´ê°€
>- ë§ì•„ ê°™ì€ ê±°ì„. 

### 3. eps
``` eps
self.eps = 1e-5
``` 

> epsê°€ ë¬´ìŠ¨ ì˜ë¯¸ì¸ë°?
>- epsilonì˜ ì•½ì 
>- "0ìœ¼ë¡œ ë‚˜ëˆ„ëŠ” ê²ƒì„ ë°©ì§€í•˜ê¸° ìœ„í•œ ì•„ì£¼ ì‘ì€ ê°’"
>- 0ê³¼ ê°€ê¹Œìš°ë©´ **ë‚˜ëˆ„ê¸° ì—°ì‚°ì´ í­ë°œí•¨**
>- ì•ˆì •ì ì¸ ê³„ì‚°ì„ í•˜ê¸° ìœ„í•´ ë¶„ëª¨ì— ë”í•˜ëŠ” ì‘ì€ ìˆ˜

### 4. scale
``` scale
self.scale = nn.Parameter(torch.ones(emb_dim))
self.shift = nn.Parameter(torch.zeros(emb_dim))
```
- `scale`ì€ Î³ (gamma) - í¬ê¸° ê°’ (ì›ë˜ í¬ê¸°ì™€ ë¹„êµ)
- ì´ˆê¸°ê°’ì€ 1, ë ˆì´ì–´ ì •ê·œí™” í›„ ìˆ˜ì •ë¨.
- nn.Parameterë¡œ ì •ì˜í•˜ë©´ í•™ìŠµ ì‹œ ì—…ë°ì´íŠ¸ ë¨

- `shift`ëŠ” Î² (beta) - ìœ„ì¹˜ ê°’ (ì›ë˜ ìœ„ì¹˜ì™€ ë¹„êµ)
- ì´ˆê¸°ê°’ì€ 0, ë ˆì´ì–´ ì •ê·œí™” í›„ ìˆ˜ì •ë¨.
- ìœ„ì™€ ë™ì¼í•˜ê²Œ nn.Parameterë¡œ ì •ì˜í•˜ë©´ í•™ìŠµ ì‹œ ì—…ë°ì´íŠ¸ ë¨.

> ê°ë§ˆ-ë² íƒ€ì™€ ìŠ¤ì¼€ì¼-ì‰¬í”„íŠ¸ì— ëŒ€í•´ì„œ ì¢€ ë” ìì„¸íˆ ì´í•´í•  ê²ƒ
#### (1) í‰ê· ê³¼ ë¶„ì‚°ì— ëŒ€í•´ì„œ ë¨¼ì € ì´í•´í•´ì•¼í•œë‹¤...
**í‰ê· (mean)**  
í‰ê· (mean)ì€ "ê°€ìš´ë° ê°’(ëŒ€í‘œ ê°’)"  

ì˜ˆì‹œ ë°ì´í„°:
```í‰ê·  ì˜ˆì‹œ ë°ì´í„°
[2, 4, 6] 
```
ì¤‘ê°„ê°’ = 4

**ë¶„ì‚°(variance)**  
ë¶„ì‚°(variance)ëŠ” "í©ì–´ì§„ ì •ë„(í¼ì ¸ìˆëŠ” ì •ë„)"

ì˜ˆì‹œ ë°ì´í„° A:
```ë¶„ì‚° ì˜ˆì‹œ ë°ì´í„° A
[3.9, 4.0, 4.1] > ë¶„ì‚°ì´ ì‘ë‹¤ 
```
ì˜ˆì‹œ ë°ì´í„° B:
```ë¶„ì‚° ì˜ˆì‹œ ë°ì´í„° A
[2, 4, 6] > ë¶„ì‚°ì´ í¬ë‹¤ 
```

#### (2) ê·¼ë… ì´ì œ Normalizationì„ í•˜ë©´ ì›ë˜ì˜ ê°’ì„ ìƒì–´ë²„ë¦°ë‹¤.
[2, 4, 6] -> [-1, 0, 1]

ì´ëŸ¬í•œ ì´ìœ ë¡œ ì´í›„, ëª¨ë¸ì´ í•„ìš”í•˜ë©´ "ì •ê·œí™”ë¥¼ ë‹¤ì‹œ í•  ìˆ˜ ìˆê²Œ ë§Œë“¤ì–´ì•¼ í•œë‹¤."
ì´ê²ƒì„ Î³(scale)Â·Î²(shift)ë¡œ ì„¤ì •í•´ë‘”ë‹¤. 

**Î³** (scale = í¬ê¸° ëŠ˜ë¦¬ê³  ì¤„ì´ê¸°)  
**Î²** (shift = ìœ„ì¹˜ ì´ë™)

## ìˆœì „íŒŒ foward pass
```foward pass
def forward(self, x):
```
- forward ë©”ì„œë“œëŠ” ìˆœì „íŒŒ(forward pass)
- xëŠ” ì…ë ¥ í…ì„œì…ë‹ˆë‹¤. ì¼ë°˜ì ìœ¼ë¡œ (batch_size, seq_len, emb_dim)

### 1. í‰ê·  ê³„ì‚°
```
mean = x.mean(dim=-1, keepdim=True)
```
- ë§ˆì§€ë§‰ ì°¨ì›(emb_dim) ê¸°ì¤€ìœ¼ë¡œ í‰ê· ì„ ê³„ì‚°
- keepdim=TrueëŠ” ê²°ê³¼ í…ì„œì˜ ì°¨ì›ì„ ìœ ì§€í•´ì„œ ë‚˜ì¤‘ì— ë¸Œë¡œë“œìºìŠ¤íŠ¸ ì—°ì‚° // ì°¨ì› ê°’ì„ ìœ ì§€í•˜ê² ë‹¨ ì˜ë¯¸
- ì˜ˆ: (batch, seq_len, emb_dim) â†’ (batch, seq_len, 1) // ë§ˆì§€ë§‰ ì°¨ì› ì“°ê² ë‹¨ ì˜ë¯¸
 
### 2. ë¶„ì‚° ê³„ì‚°
``` ë¶„ì‚° ê³„ì‚°
var = x.var(dim=-1, keepdim=True, unbiased=False)
```
- ë§ˆì§€ë§‰ ì°¨ì›(emb_dim) ê¸°ì¤€ìœ¼ë¡œ í‰ê· ì„ ê³„ì‚°
- unbased ->  ëª¨ì§‘ë‹¨ ë¶„ì„ ì„ íƒ /

### 3. ì •ê·œí™”
``` Normalization
norm_x = (x - mean) / torch.sqrt(var + self.eps)
```
- ì…ë ¥ì„ ì •ê·œí™”(normalize)í•©ë‹ˆë‹¤.
- í‰ê· ì„ ë¹¼ê³ , ë¶„ì‚°ì˜ ì œê³±ê·¼(í‘œì¤€í¸ì°¨)ìœ¼ë¡œ ë‚˜ëˆ•ë‹ˆë‹¤.
- epsë¥¼ ë”í•´ì„œ 0ìœ¼ë¡œ ë‚˜ëˆ„ëŠ” ë¬¸ì œë¥¼ ë°©ì§€í•©ë‹ˆë‹¤.
- ì´ë ‡ê²Œ í•˜ë©´ norm_xì˜ ë§ˆì§€ë§‰ ì°¨ì› ê°’ë“¤ì´ í‰ê·  0, ë¶„ì‚° 1ì´ ë©ë‹ˆë‹¤.

### 4. ì ìš©
``` shiftì™€ scale ì ìš©
return self.scale * norm_x + self.shift
```

# [CD] 3. GELU

GELUëŠ” ReLUë¶€ë„ Tranformerì— ë” ì í•©í•œ í•¨ìˆ˜

ì…ë ¥ xê°€ í´ìˆ˜ë¡ ë” í†µê³¼ì‹œí‚¤ê³ , 
xê°€ ì‘ì„ ìˆ˜ë¡ í™•ë¥ ì ìœ¼ë¡œ 0 ê·¼ì²˜ì— ë³´ë‚´ëŠ” í•¨ìˆ˜

## ì „ì²´ ì½”ë“œ
```GelU
class GELU(nn.Module):
    def __init__(self):
        super().__init__()

    def forward(self, x):
        return 0.5 * x * (1 + torch.tanh(
            torch.sqrt(torch.tensor(2.0 / torch.pi)) *
            (x + 0.044715 * torch.pow(x, 3))
        ))
```

# [CD] 4. FeedFoward(FFN)

ì—¬ê¸°ì˜ FFNì€ ë…ë¦½ì ìœ¼ë¡œ ì ìš©ë˜ëŠ” ì‘ì€ MLP

## Code Overview
```
class FeedForward(nn.Module):
    def __init__(self):
        super().__init__()
        self.layers = nn.Sequential(
            nn.Linear(EMB_DIM, 4 * EMB_DIM),
            GELU(),
            nn.Linear(4 * EMB_DIM, EMB_DIM),
        )
```

## Expansion Embedding
```
nn.Linear(EMB_DIM â†’ 4 Ã— EMB_DIM),
GELU(),
nn.Linear(4Ã—EMB_DIM â†’ EMB_DIM)
```

- ì…ë ¥ ì„ë² ë”© í¬ê¸°ë¥¼ 4ë°°ë¡œ í™•ì¥í•˜ëŠ” ë‹¨ê³„
- GELU() ì ìš©
- ì„ë² ë”© í¬ê¸°ë¥¼ ë‹¤ì‹œ ì›ë˜ ìƒíƒœë¡œ ì¶•ì†Œ (Projection Back)
